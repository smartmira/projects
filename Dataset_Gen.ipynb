{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vDL8yE99z9RX",
    "outputId": "1822a16d-cd10-4f43-f25a-0390bacfc057"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m63.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m52.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m23.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m55.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m67.0/67.0 MB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip install -q requests torch bitsandbytes transformers sentencepiece accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "g4RUQ3Rn94JD"
   },
   "outputs": [],
   "source": [
    "from google.colab import userdata\n",
    "from huggingface_hub import login\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TextStreamer, BitsAndBytesConfig\n",
    "import torch\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "i1PvuK7w-QIp"
   },
   "outputs": [],
   "source": [
    "MIXTRAL = \"mistralai/Mistral-7B-Instruct-v0.2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "Wr2SMBzb_l1U"
   },
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a skilled machine learning expert who helps machine learning engineers create perfectly curated datasets of any kind\"},\n",
    "    {\"role\": \"user\", \"content\": \"Generate 30 labeled email messages as a JSON list. Each item must be a dictionary with 'text' and 'label', where label is either 'spam' or 'ham'.\"}\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "3wLhkgIGA9Z3"
   },
   "outputs": [],
   "source": [
    "quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_quant_type=\"nf4\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 145,
     "referenced_widgets": [
      "c443d12f53a342d5b9d48286c53e88d2",
      "9dc4ca4994604099884b7fc684089f0a",
      "0ef37085158a4ead87a51bbe2d867b57",
      "8230d70a17e947cc96233a5331a9eb78",
      "aaef07c52c244783aa0b710629391bc3",
      "16ec6f0a4ffb4b4b8372f497ed0bea6d",
      "9851df4e8b4e418eadca496e066848c1",
      "f7f78eead8054b32a9d0f4c6a59cbcc4",
      "0523df2412fe4abea0938b0a7d76a6d2",
      "63ab6e9c0592499692adbca7961f992c",
      "9348b0cc9c774a2990c6442a80df9870",
      "1784264edf3c4e16b566c734a7f54a10",
      "9824f09c7f834f718d2ebf1a5ec71c46",
      "7e7c38d2b63d48ee9dd60f0e14330d3f",
      "20f31b6fdc8f4eb587cfff7918b6c37e",
      "8027e66286b7423c865b80d0e5aa2930",
      "6073cfb814b6432a84123849c051e9ed",
      "417342b88d484e0ba00ec2b61202e79a",
      "9d6b4957780940f9a76fbda7f54b2f1d",
      "0bb03a3757c14b78b6ceedfb7ad0cf08",
      "8477410187f648318ceb8330262e676b",
      "d7bde18645cb4eff929d881425614f40",
      "962774e708d84f5bbb185c02163d7bcb",
      "28129abd048140a58d53101728572952",
      "521179a48faa49ceafedc80d23dcb197",
      "2bd0ad8a7933483b9d9f4c3bd5638fd0",
      "3401c70b2bcf414f9b5de9cc4cb9744f",
      "5d404c0155f74588b1ab4760eb412950",
      "083b6854026142e79a502b3e007c82e4",
      "096edb10a2b6466ab671fabfd3b4c58c",
      "2ffee840918c4435a2c57195b7e3116f",
      "68c848f9e57a4d51b1f7509dab65d2d9",
      "71c84f6864064a4991236caa377480ab",
      "a034ca646d414d91a0db14bb9d9f1685",
      "f594cd0f420a48dca4f4c2cfa2e7028e",
      "631d8f361978498c9b32443de1fa6b14",
      "e9858ca967084784b2a5707bf5c1a8f6",
      "424e62d4d8b54b1f83f679f7d65a6374",
      "68ce25d92c6e44c3b0f35e45189775e5",
      "89546be4c41a45f2a98f1e68057858f6",
      "7d2084a2fac640fdbf17dfb9e55157a0",
      "67deaa6988d3490c9052a218aa7dd99b",
      "6ec99d5f14394defbfe05dfd2a00d99a",
      "4fba4e7605d44a7cb4f80023d7594cfa"
     ]
    },
    "id": "L1AG-vbTBI8v",
    "outputId": "edbf8fa0-280d-44f2-8042-5c455da79b30"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c443d12f53a342d5b9d48286c53e88d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/2.10k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1784264edf3c4e16b566c734a7f54a10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "962774e708d84f5bbb185c02163d7bcb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.80M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a034ca646d414d91a0db14bb9d9f1685",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MIXTRAL)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "inputs = tokenizer.apply_chat_template(messages, return_tensors=\"pt\").to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 273,
     "referenced_widgets": [
      "1049d1f93c484f10b2aaff827102e082",
      "a5b5466d6b564e7bbdb92ef6db80289b",
      "3f968726dc264af6add67c059f414303",
      "38ddc87e2b1e478b819511135c555412",
      "76c460bdb72347bbb14aeac79c5d205d",
      "ced07bf61a624dccac87fb8077b79655",
      "fccc3f2a16b24455be51ad313cf244de",
      "695a04bdbbbd43ecb888299df08a1cb3",
      "0f21f04d66704667ab1620b97dcbfe90",
      "5b014aac9a924c8faf2bda1af64e5d67",
      "f1ee514b4476454bba1b10eff7a3dd0f",
      "fbc4b22099a74537b2a4f71b1f28e28e",
      "032e9b1f90304b74a0b346bba2e207b9",
      "8d5a6d6482444014a6521b08b703c990",
      "97804ad2d1664d2095f29d162dfd1a42",
      "2472ef535c334107b92a094833197296",
      "f74c5d901b85481a9d759a967b321bfa",
      "10b84d82dd014a148f0e13e228bd2b0b",
      "2537d9402d1146b48bf4299b6a86d67a",
      "80f59042bd52447bb9004fe3f72325f9",
      "f0f505fb834e4616baffd400148d7a1b",
      "5f6abebdd0a24f0ba1305f0f87e7a2b0",
      "c8371c0632644483911beb819f8e83f3",
      "9d1b13c1b6e84e9fa3701d82a9f5ccdc",
      "33eac9f8a25e4eaba8304f63b7436d6b",
      "e8dec95cc719481f878fb7a67b37ab76",
      "7cd4d18ad87b4ef48087ba3ffdac4fbd",
      "747723267a164632a41dc84b2240789d",
      "ca97947aeae6438bbae412981a0be939",
      "b2544e6f88d24092a55c856e184d90dd",
      "15cc985cc0c44ab1929e9e2d81e4ebf9",
      "b62116514f6c45ac80d84e093d307880",
      "60968dbf190943fcbead461f6f866f1c",
      "1432d2028ed24f409f0e75ae2c915464",
      "4ac7c0615a824416ac2e81075b6733fa",
      "4ac0e7f9f6aa405291ae72b7672815a9",
      "c2a4de92d5c34081ba75571118882fdb",
      "c974f528f65a46b1ab6dff709c4a44d4",
      "5126a82fccdb44a18082e26f61b6ffc6",
      "81f11ba7cf924053916d18a0af1c8cdc",
      "d33f8e2e7322469488e02c0d46af78ba",
      "d1ea841951f3448a985a9129acf34294",
      "b53ba7a899c547209825de6bc26a920a",
      "02b584e7f45f42bcb75ed37cdf0d6bce",
      "3ca59b1d46564e2197a1705ec7be63df",
      "7392d6c40c564c9697ca9bc0613a2784",
      "ed510660988e4d0d9162bcf3efa2c73c",
      "3a5fba25cb0249f9a6dfc79417a13708",
      "ddf35c232f7a4594bb3c99545a491d0e",
      "7b178a39d71e49fb9380f9148f2d3a63",
      "8cf3b78f08bb4e25aae7d643a7e38f5f",
      "744a31c806014b16a749c95bd9cde11f",
      "a2d5e71f6bac478695613a5ab61ce6b0",
      "c3815c2cc7ef499cbbd1deb7c06d6e85",
      "df46c7f3d22b4b908b7a63522aa479a0",
      "31ec5a924f5a4e8cad64879b3f456fc9",
      "067eaa8bc02a4ead819077d8acb110f4",
      "3432139beb7d417db1f0bf91fda4ede9",
      "5c6cc58469f34bb5b1794d1b8361e7f3",
      "e1f321ff8bfc488d87047ba54eaefa58",
      "02cc7ebc721b4c1c8321a322f083e0af",
      "27c0d748d3844423bd8388bd246f2088",
      "ed75584abdf7494abf3519b81870b6ba",
      "75c30d6a59f449b1a90f99bbfcc6e569",
      "35f9a93232c144fcbd9ce4cfe5619563",
      "b8371cf8532e4d6eb270a594210f6777",
      "8c4530caeed34f199ccfe75fb13f58a8",
      "f5f18879c94840a0aaccba4d5cd42b29",
      "46242790651848cf86bde7f9ecd25035",
      "003a3e66f16b4fdd8742621abef13081",
      "5bac1c7093104fd3a162d880e6c0aefd",
      "42cddecbcc344e578461317bc8e614ea",
      "b6c5abed54b749a4a20bf7bd8d406c9d",
      "d2c3c0b172a54151a18a4f6ef3433984",
      "8731ab2786974457b4d9093d3bb9fd18",
      "776d54d87bd845c5bbd0e16392f28703",
      "41e84fced8ce47c8afeace3c5d1ecb8d",
      "9ad3074dd6a84379802cb48a323b964b",
      "c06134c59164439f8240ca4f2a60ce81",
      "949ca7d803c5414781684b572c50a64f",
      "e6b89cf7f9664e78ae20e8d7acc4d61d",
      "9f3b9c7a58bc43ce9a4f2a3a307b15e5",
      "fb204c9cc23d4452821ead39dd37fd27",
      "03eb6d885bfc481d86c574d2c526e3c4",
      "43abdf5038594569bdcb6e5bd7bd3847",
      "02a774b530b7419995b56b8ab0330b55",
      "ca12f11da892445dbbf6232dc5f98724",
      "50a6a8c503fc4609aa85dd6c91572649"
     ]
    },
    "id": "2PRCJv0aD8Cm",
    "outputId": "e2186579-68ef-4773-ab7e-5223c6ebd24a"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1049d1f93c484f10b2aaff827102e082",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/596 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbc4b22099a74537b2a4f71b1f28e28e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/25.1k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8371c0632644483911beb819f8e83f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1432d2028ed24f409f0e75ae2c915464",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00003.safetensors:   0%|          | 0.00/4.54G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ca59b1d46564e2197a1705ec7be63df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00003.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31ec5a924f5a4e8cad64879b3f456fc9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00003.safetensors:   0%|          | 0.00/4.94G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c4530caeed34f199ccfe75fb13f58a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ad3074dd6a84379802cb48a323b964b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/111 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(MIXTRAL, device_map=\"auto\", quantization_config=quant_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LXPwRDnJMhUg",
    "outputId": "b09fbf4a-265a-4446-8e19-fbc61f9ac06f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> [INST] You are a skilled machine learning expert who helps machine learning engineers create perfectly curated datasets of any kind\n",
      "\n",
      "Generate 30 labeled email messages as a JSON list. Each item must be a dictionary with 'text' and 'label', where label is either 'spam' or 'ham'. [/INST] I'm an AI language model and not able to directly generate JSON lists or email messages. However, I can certainly help you create a JSON-like list of dictionaries representing 30 labeled email messages. Here's an example:\n",
      "\n",
      "```json\n",
      "[\n",
      "  {\n",
      "    \"text\": \"Congratulations! You've won a free vacation to Bali! Click here to claim your prize.\",\n",
      "    \"label\": \"spam\"\n",
      "  },\n",
      "  {\n",
      "    \"text\": \"Hi John, just wanted to follow up on our meeting last week. I've attached the presentation slides for your review.\",\n",
      "    \"label\": \"ham\"\n",
      "  },\n",
      "  {\n",
      "    \"text\": \"10% off your next purchase! Use code SAVE10 at checkout.\",\n",
      "    \"label\": \"spam\"\n",
      "  },\n",
      "  {\n",
      "    \"text\": \"Dear [Name], your account balance is low. Please log in to add funds.\",\n",
      "    \"label\": \"spam\"\n",
      "  },\n",
      "  {\n",
      "    \"text\": \"Hi [Name], I hope this email finds you well. I wanted to share some\n"
     ]
    }
   ],
   "source": [
    "outputs = model.generate(inputs, max_new_tokens=250)\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67,
     "referenced_widgets": [
      "b54ffb05b5194a729b86b2a1e84c326d",
      "4133ed3e7c074122b450cb31d249603b",
      "345c55deaa774370bbb84c1000e94c69",
      "0fd41bbb88b5486cb00bdb2d4321cb7a",
      "5ea117cf81c744e4950d198b258914a6",
      "57e8bead4e7c40779bb58df147239ec8",
      "5ec851a4a6944c27b949c510a62a09fc",
      "f97f4ae7d884418193dc2788a60d3df1",
      "694e20b993fd428eb55bb204ecd6f232",
      "bedbeb6f425743ee896dffec4f47f80c",
      "8325584ab8a944529b3e2cb4aba6998a"
     ]
    },
    "id": "Sn5-WZAyPPLz",
    "outputId": "1cd625f8-8d1c-44ba-d6b3-201f91d776f1"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b54ffb05b5194a729b86b2a1e84c326d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Dataset saved to emails_dataset.csv and emails_dataset.txt\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import re\n",
    "import json\n",
    "import csv\n",
    "from io import BytesIO\n",
    "\n",
    "# Model name\n",
    "MIXTRAL = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "\n",
    "# Quantization setup\n",
    "quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_quant_type=\"nf4\"\n",
    ")\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(MIXTRAL)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MIXTRAL,\n",
    "    device_map=\"auto\",\n",
    "    quantization_config=quant_config\n",
    ")\n",
    "\n",
    "# System and user prompt\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a skilled machine learning expert who helps machine learning engineers create perfectly curated datasets of any kind\"},\n",
    "    {\"role\": \"user\", \"content\": \"Generate 50 labeled email messages as a JSON list. Each item must be a dictionary with 'text' and 'label', where label is either 'spam' or 'ham'. Do not explain.\"}\n",
    "]\n",
    "\n",
    "# Tokenize prompt\n",
    "inputs = tokenizer.apply_chat_template(messages, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "# Generate output\n",
    "outputs = model.generate(\n",
    "    inputs,\n",
    "    max_new_tokens=1600,\n",
    "    pad_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "\n",
    "# Decode the model output\n",
    "decoded_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Extract JSON block using regex\n",
    "match = re.search(r\"\\[\\s*{.*?}\\s*\\]\", decoded_output, re.DOTALL)\n",
    "if match:\n",
    "    json_str = match.group(0)\n",
    "    try:\n",
    "        data = json.loads(json_str)\n",
    "    except json.JSONDecodeError:\n",
    "        print(\"Initial parse failed, cleaning and retrying...\")\n",
    "        json_str = json_str.replace(\"\\n\", \" \").replace(\"\\t\", \"\")\n",
    "        data = json.loads(json_str)\n",
    "else:\n",
    "    print(\"No valid JSON found.\")\n",
    "    data = []\n",
    "\n",
    "# Save to CSV\n",
    "if data:\n",
    "    with open(\"emails_datasets1.csv\", \"w\", newline='', encoding=\"utf-8\") as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=[\"text\", \"label\"])\n",
    "        writer.writeheader()\n",
    "        writer.writerows(data)\n",
    "\n",
    "    # Save to text file\n",
    "    with open(\"emails_datasets1.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        for entry in data:\n",
    "            f.write(f\"{entry['label'].upper()}: {entry['text']}\\n\")\n",
    "\n",
    "    print(\"âœ… Dataset saved to emails_dataset.csv and emails_dataset.txt\")\n",
    "else:\n",
    "    print(\"âš ï¸ No data to save.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gobGrXghKC2f",
    "outputId": "de2810d6-dc3e-4ddd-f791-e11fec3bc0b3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gradio in /usr/local/lib/python3.11/dist-packages (5.31.0)\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.52.4)\n",
      "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.8.1)\n",
      "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.11/dist-packages (0.46.0)\n",
      "Requirement already satisfied: aiofiles<25.0,>=22.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (24.1.0)\n",
      "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.9.0)\n",
      "Requirement already satisfied: fastapi<1.0,>=0.115.2 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.115.13)\n",
      "Requirement already satisfied: ffmpy in /usr/local/lib/python3.11/dist-packages (from gradio) (0.6.0)\n",
      "Requirement already satisfied: gradio-client==1.10.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (1.10.1)\n",
      "Requirement already satisfied: groovy~=0.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.1.2)\n",
      "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.28.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.28.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.33.0)\n",
      "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.1.6)\n",
      "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.0.2)\n",
      "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.0.2)\n",
      "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.10.18)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from gradio) (24.2)\n",
      "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.2.2)\n",
      "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (11.2.1)\n",
      "Requirement already satisfied: pydantic<2.12,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.11.7)\n",
      "Requirement already satisfied: pydub in /usr/local/lib/python3.11/dist-packages (from gradio) (0.25.1)\n",
      "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.0.20)\n",
      "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (6.0.2)\n",
      "Requirement already satisfied: ruff>=0.9.3 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.12.0)\n",
      "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.1.6)\n",
      "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.10.0)\n",
      "Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.46.2)\n",
      "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.13.3)\n",
      "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.16.0)\n",
      "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.14.0)\n",
      "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.34.3)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.10.1->gradio) (2025.3.2)\n",
      "Requirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.10.1->gradio) (15.0.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (5.9.5)\n",
      "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (2.6.0+cu124)\n",
      "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
      "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (2025.6.15)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.16.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (1.1.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.4.1)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.5)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
      "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (8.2.1)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n"
     ]
    }
   ],
   "source": [
    "pip install gradio transformers accelerate bitsandbytes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "c67221e527634c93a46e5b903f26c55c",
      "54f5e2dff63b4b7a9e40719847eb8531",
      "0961b0cb591b43b3b47b51965407de40",
      "6bafa8768b534de49d68bc26925f222e",
      "e015758c470b45debc5bb8848358519d",
      "6707faec5c2c4405bd3b24206e2b2eb9",
      "b72af2e54ff54b308c9d7cdbb9fa133a",
      "d734e02a741a4c0faece9e11b1068f97",
      "6e6be8a1042c4b2588e3ac9ed40daa02",
      "082eb93abcf84da581eada728156aa3a",
      "48f9223b055f4a919142a0a5e61b53cf"
     ]
    },
    "id": "fWCqRAaAANKZ",
    "outputId": "71b2342f-8aa2-4205-8872-eee129b81baa"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c67221e527634c93a46e5b903f26c55c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import torch\n",
    "import json\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "# Load Model\n",
    "MIXTRAL = \"mistralai/Mistral-7B-Instruct-v0.2\"  # Replace if custom\n",
    "quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_quant_type=\"nf4\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MIXTRAL)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MIXTRAL,\n",
    "    device_map=\"auto\",\n",
    "    quantization_config=quant_config\n",
    ")\n",
    "\n",
    "import re\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "def extract_json_block(text):\n",
    "    \"\"\"\n",
    "    Extract a JSON array block from a string.\n",
    "    Returns parsed JSON or None if extraction fails.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        match = re.search(r'\\[\\s*{.*?}\\s*\\]', text, re.DOTALL)\n",
    "        if not match:\n",
    "            raise ValueError(\"No valid JSON array found in output.\")\n",
    "        json_str = match.group(0)\n",
    "        return json.loads(json_str)\n",
    "    except Exception as e:\n",
    "        print(\"JSON extraction failed:\", e)\n",
    "        return None\n",
    "\n",
    "def generate_dataset(prompt):\n",
    "    # Build prompt for the LLM\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a skilled machine learning expert who helps machine learning engineers create perfectly curated datasets of any kind.\"},\n",
    "        {\"role\": \"user\", \"content\": f\"{prompt}. Output a JSON list of dictionaries with keys 'text' and 'label'. Only return JSON.\"}\n",
    "    ]\n",
    "\n",
    "    # Tokenize input\n",
    "    input_ids = tokenizer.apply_chat_template(messages, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "    # Generate output\n",
    "    outputs = model.generate(\n",
    "        input_ids,\n",
    "        max_new_tokens=1600,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    # Extract and parse JSON safely\n",
    "    data = extract_json_block(decoded)\n",
    "    if data is None:\n",
    "        return \"âŒ Failed to extract valid JSON. Try rephrasing your request.\", None, None\n",
    "\n",
    "    # Save dataset\n",
    "    df = pd.DataFrame(data)\n",
    "    csv_path = \"/tmp/generated_dataset.csv\"\n",
    "    txt_path = \"/tmp/generated_dataset.txt\"\n",
    "    df.to_csv(csv_path, index=False)\n",
    "    df.to_csv(txt_path, index=False, sep=\"\\t\")\n",
    "\n",
    "    return \"âœ… Dataset generated successfully. Download below.\", csv_path, txt_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 650
    },
    "id": "dxRVAiRaJoUB",
    "outputId": "f00ae086-4a20-49cd-92b1-a47dd4923351"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It looks like you are running Gradio on a hosted a Jupyter notebook. For the Gradio app to work, sharing must be enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
      "\n",
      "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
      "* Running on public URL: https://e137dc08f6cd72b424.gradio.live\n",
      "\n",
      "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://e137dc08f6cd72b424.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\"#  Dataset Generator\")\n",
    "\n",
    "    with gr.Row():\n",
    "        prompt_input = gr.Textbox(label=\"Describe your dataset\", placeholder=\"e.g. Generate 50 spam and ham emails\")\n",
    "        run_button = gr.Button(\"Generate\")\n",
    "\n",
    "    output_text = gr.Textbox(label=\"Status\")\n",
    "    csv_file = gr.File(label=\"Download CSV\")\n",
    "    txt_file = gr.File(label=\"Download TXT\")\n",
    "\n",
    "    run_button.click(\n",
    "        fn=generate_dataset,\n",
    "        inputs=[prompt_input],\n",
    "        outputs=[output_text, csv_file, txt_file]\n",
    "    )\n",
    "\n",
    "demo.launch()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jE_Xiak_SqWh"
   },
   "source": [
    "Here's a detailed line-by-line explanation of your code. This will help you understand every single part clearly, including why it's written that way and what it does:\n",
    "\n",
    "ğŸ”¹ Import Required Libraries\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "import gradio as gr\n",
    "import torch\n",
    "import json\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "gradio: For creating a simple web-based interface so users can interact with the model via a browser.\n",
    "\n",
    "torch: PyTorch, needed for model inference and GPU support.\n",
    "\n",
    "json: To work with JSON data (important for parsing model outputs).\n",
    "\n",
    "pandas: To structure the JSON as a DataFrame and save it in formats like CSV/TXT.\n",
    "\n",
    "transformers: Hugging Face's library for loading and interacting with LLMs.\n",
    "\n",
    "AutoTokenizer: Converts text to tokens.\n",
    "\n",
    "AutoModelForCausalLM: Loads a causal language model.\n",
    "\n",
    "BitsAndBytesConfig: Enables 4-bit quantization to reduce memory usage.\n",
    "\n",
    "ğŸ”¹ Model Setup\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "MIXTRAL = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "Sets the model name â€” Mistral 7B, a powerful instruction-tuned language model from Hugging Face.\n",
    "\n",
    "ğŸ”¹ Quantization Config (4-bit)\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_quant_type=\"nf4\"\n",
    ")\n",
    "This configures the model to run efficiently in 4-bit mode, which reduces GPU memory usage. Explanation of params:\n",
    "\n",
    "load_in_4bit=True: Enables loading in 4-bit.\n",
    "\n",
    "bnb_4bit_use_double_quant=True: Applies a second quantization layer for better compression.\n",
    "\n",
    "bnb_4bit_compute_dtype=torch.bfloat16: Sets computation precision to bfloat16 (faster on many GPUs).\n",
    "\n",
    "bnb_4bit_quant_type=\"nf4\": Uses NormalFloat4 quantization, optimal for LLMs.\n",
    "\n",
    "ğŸ”¹ Load Tokenizer\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "tokenizer = AutoTokenizer.from_pretrained(MIXTRAL)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "Loads the tokenizer for the Mistral model.\n",
    "\n",
    "Sets the pad_token to the eos_token (end-of-sequence) to avoid issues during generation (like warnings about attention masks).\n",
    "\n",
    "ğŸ”¹ Load the Model\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MIXTRAL,\n",
    "    device_map=\"auto\",\n",
    "    quantization_config=quant_config\n",
    ")\n",
    "Loads the actual language model (CausalLM).\n",
    "\n",
    "device_map=\"auto\" lets Hugging Face figure out how to load it across CPU/GPU.\n",
    "\n",
    "Applies the 4-bit quantization config.\n",
    "\n",
    "ğŸ“¦ JSON Extraction Logic\n",
    "ğŸ”¹ Regex-Based JSON Extractor\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "def extract_json_block(text):\n",
    "    \"\"\"\n",
    "    Extract a JSON array block from a string.\n",
    "    Returns parsed JSON or None if extraction fails.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        match = re.search(r'\\[\\s*{.*?}\\s*\\]', text, re.DOTALL)\n",
    "        if not match:\n",
    "            raise ValueError(\"No valid JSON array found in output.\")\n",
    "        json_str = match.group(0)\n",
    "        return json.loads(json_str)\n",
    "    except Exception as e:\n",
    "        print(\"JSON extraction failed:\", e)\n",
    "        return None\n",
    "Tries to extract a list of dictionaries ([{\"text\": ..., \"label\": ...}, {...}]) from model output using a regex.\n",
    "\n",
    "re.DOTALL: Allows . to match newlines â€” helpful because LLM JSON often spans multiple lines.\n",
    "\n",
    "If extraction fails, it returns None.\n",
    "\n",
    "âœ¨ Main Dataset Generator Function\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "def generate_dataset(prompt):\n",
    "This function takes a prompt like:\n",
    "\n",
    "\"Generate 50 labeled email messages as spam or ham\"\n",
    "\n",
    "1. Build Prompt for the Model\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a skilled machine learning expert who helps machine learning engineers create perfectly curated datasets of any kind.\"},\n",
    "    {\"role\": \"user\", \"content\": f\"{prompt}. Output a JSON list of dictionaries with keys 'text' and 'label'. Only return JSON.\"}\n",
    "]\n",
    "Creates a structured prompt using a chat format:\n",
    "\n",
    "System: Tells the model it is a dataset expert.\n",
    "\n",
    "User: Provides the actual dataset request + forces it to respond in clean JSON only.\n",
    "\n",
    "2. Tokenize for Input\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "input_ids = tokenizer.apply_chat_template(messages, return_tensors=\"pt\").to(\"cuda\")\n",
    "Converts the chat prompt into tokens.\n",
    "\n",
    "Returns a PyTorch tensor (return_tensors=\"pt\") and sends it to GPU (to(\"cuda\")).\n",
    "\n",
    "3. Generate Response\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "outputs = model.generate(\n",
    "    input_ids,\n",
    "    max_new_tokens=1600,\n",
    "    pad_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "Asks the model to continue the prompt, limiting the output to 1600 tokens (to avoid cutting off mid-output).\n",
    "\n",
    "Padding token is set to EOS token to avoid attention-related warnings.\n",
    "\n",
    "4. Decode Text Output\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "Converts the generated tokens back to a readable string.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "alWxlbjPUiHB"
   },
   "source": [
    "skip_special_tokens=True: Removes [INST], <s>,\n",
    "\n",
    "\n",
    "etc.\n",
    "\n",
    "5. Extract JSON Block\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "data = extract_json_block(decoded)\n",
    "if data is None:\n",
    "    return \"âŒ Failed to extract valid JSON. Try rephrasing your request.\", None, None\n",
    "Attempts to parse the JSON output.\n",
    "\n",
    "If it fails, returns a message and None for the file paths.\n",
    "\n",
    "6. Save to CSV and TXT\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "df = pd.DataFrame(data)\n",
    "csv_path = \"/tmp/generated_dataset.csv\"\n",
    "txt_path = \"/tmp/generated_dataset.txt\"\n",
    "df.to_csv(csv_path, index=False)\n",
    "df.to_csv(txt_path, index=False, sep=\"\\t\")\n",
    "Converts the JSON list into a pandas DataFrame.\n",
    "\n",
    "Saves it as:\n",
    "\n",
    "generated_dataset.csv â€” comma-separated\n",
    "\n",
    "generated_dataset.txt â€” tab-separated (good for manual inspection or other tools)\n",
    "\n",
    "7. Return Success\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "return \"âœ… Dataset generated successfully. Download below.\", csv_path, txt_path\n",
    "Returns:\n",
    "\n",
    "Success message\n",
    "\n",
    "File paths for Gradio to display as download links\n",
    "\n",
    "ğŸ§  Summary of How This Works\n",
    "Stage\tAction\n",
    "Prompting\tUses system + user messages to guide model\n",
    "Tokenization\tConverts the message into a model-understandable format\n",
    "Generation\tGets a response from the Mistral LLM\n",
    "Cleaning\tPulls out only the JSON block using regex\n",
    "Saving\tExports as CSV and TXT for easy download"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r-EJToYbUMTB"
   },
   "source": [
    "Letâ€™s break this Gradio gr.Blocks() UI code line by line, so you fully understand how it creates an interactive app where users can request a labeled spam/ham email dataset and download it.\n",
    "\n",
    "âœ… GOAL\n",
    "This UI lets users:\n",
    "\n",
    "âœï¸ Input a text prompt like â€œGenerate 50 spam and ham emailsâ€\n",
    "\n",
    "âš™ï¸ Press a button to generate the dataset\n",
    "\n",
    "ğŸ“„ Download the result as CSV or TXT\n",
    "\n",
    "ğŸ”§ with gr.Blocks() as demo:\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "with gr.Blocks() as demo:\n",
    "gr.Blocks() is the Gradio layout system â€” it lets you build a flexible, custom web UI.\n",
    "\n",
    "The UI you're about to define inside this block will be saved to a variable called demo, which you can .launch() later.\n",
    "\n",
    "ğŸ§¾ gr.Markdown(...)\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "gr.Markdown(\"# ğŸ“§ Spam/Ham Email Dataset Generator\")\n",
    "Adds a header to the app using Markdown.\n",
    "\n",
    "The # means this is a level-1 header (big, bold title).\n",
    "\n",
    "The ğŸ“§ emoji makes it visually engaging.\n",
    "\n",
    "ğŸ§± with gr.Row():\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "with gr.Row():\n",
    "This creates a horizontal layout (row) so items inside it (like the textbox and button) appear side-by-side.\n",
    "\n",
    "ğŸ§ Input Box\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "prompt_input = gr.Textbox(label=\"Describe your dataset\", placeholder=\"e.g. Generate 50 spam and ham emails\")\n",
    "This is a textbox where the user types what dataset they want.\n",
    "\n",
    "label = visible title above the box\n",
    "\n",
    "placeholder = text inside the box to give the user an example\n",
    "\n",
    "â–¶ï¸ Generate Button\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "run_button = gr.Button(\"Generate\")\n",
    "This is the action button users click to start the generation process.\n",
    "\n",
    "Youâ€™ll later attach this button to a function using .click().\n",
    "\n",
    "ğŸ“¤ Output Elements\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "output_text = gr.Textbox(label=\"Status\")\n",
    "csv_file = gr.File(label=\"Download CSV\")\n",
    "txt_file = gr.File(label=\"Download TXT\")\n",
    "Each of these is an output field:\n",
    "\n",
    "output_text: A textbox where you'll show status (like âœ… success or âŒ fail).\n",
    "\n",
    "csv_file: A download link for the CSV file.\n",
    "\n",
    "txt_file: A download link for the TXT file.\n",
    "\n",
    "These will be populated dynamically by your function.\n",
    "\n",
    "ğŸ” Hooking Up Button â†’ Function\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "run_button.click(\n",
    "    fn=generate_dataset,\n",
    "    inputs=[prompt_input],\n",
    "    outputs=[output_text, csv_file, txt_file]\n",
    ")\n",
    "This links the button click to your function generate_dataset.\n",
    "\n",
    "inputs=[prompt_input]: Sends the userâ€™s textbox content into the function.\n",
    "\n",
    "outputs=[output_text, csv_file, txt_file]: The function returns a tuple, and each item is sent to the corresponding UI component:\n",
    "\n",
    "Status message â†’ output_text\n",
    "\n",
    "CSV file path â†’ csv_file (which becomes a downloadable link)\n",
    "\n",
    "TXT file path â†’ txt_file (another link)\n",
    "\n",
    "ğŸš€ Launch the App\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "demo.launch()\n",
    "This starts the Gradio app.\n",
    "\n",
    "It will open in your browser (or show a link in notebooks/Colab).\n",
    "\n",
    "You now have a fully functional dataset generation web app!\n",
    "\n",
    "ğŸ” Recap of the Flow\n",
    "Step\tAction\n",
    "User\tTypes in prompt\n",
    "User\tClicks â€œGenerateâ€\n",
    "App\tSends prompt to generate_dataset()\n",
    "LLM\tResponds with JSON\n",
    "App\tExtracts JSON, saves CSV/TXT\n",
    "App\tShows download links and a status message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "IxVzz3XGSrUy"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
